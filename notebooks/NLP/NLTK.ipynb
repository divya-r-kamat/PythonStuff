{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Cleaning\n",
    "\n",
    "Data scientists spend most of their time not on modeling but on cleaning and exploring the data. Furthermore, different approaches in text cleaning can lead to very diverse results during model training.\n",
    "\n",
    "Cleaning text-data is a typical pre-processing task for data science and machine learning. It consists of getting rid of the less useful parts of text through stopword removal, dealing with capitalization, special characters and other details.\n",
    "\n",
    "Today we’re going to do cleaning text from Kafka’s famous book Metamorphosis, as described in Jason Brownlee’s post.\n",
    "https://machinelearningmastery.com/clean-text-machine-learning-python/\n",
    "\n",
    "#### Take a look at the data: \n",
    "\n",
    "- explore its main characteristics like size and structure to see how sentences, paragraphs, text are built.\n",
    "- Understand how much of this data is useful for your needs.\n",
    "- Review the text to see what exactly might help."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Read the file and load the data\n",
    "#This loads the whole file into memory ready to work with\n",
    "\n",
    "filename = r'C:\\Users\\divyakamat\\data\\DataSets\\nlp\\metamorphosis_clean.txt'\n",
    "\n",
    "with open(filename,'rt') as file:\n",
    "    text=file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ï»¿one', 'morning,', 'when', 'gregor', 'samsa', 'woke', 'from', 'troubled', 'dreams,', 'he', 'found', 'himself', 'transformed', 'in', 'his', 'bed', 'into', 'a', 'horrible', 'vermin.', 'he', 'lay', 'on', 'his', 'armour-like', 'back,', 'and', 'if', 'he', 'lifted', 'his', 'head', 'a', 'little', 'he', 'could', 'see', 'his', 'brown', 'belly,', 'slightly', 'domed', 'and', 'divided', 'by', 'arches', 'into', 'stiff', 'sections.', 'the', 'bedding', 'was', 'hardly', 'able', 'to', 'cover', 'it', 'and', 'seemed', 'ready', 'to', 'slide', 'off', 'any', 'moment.', 'his', 'many', 'legs,', 'pitifully', 'thin']\n"
     ]
    }
   ],
   "source": [
    "#Next clean text to convert raw text into a list of words - split by whitespace\n",
    "# We use split method here and the words are stored in a list\n",
    "\n",
    "words = text.split()\n",
    "#Convert all words to lower case by calling the lower() function\n",
    "words = [word.lower() for word in words]\n",
    "print(words[:70])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We can see that the punctuations are preserved using split  method, which is nice\n",
    "- Also, end of sentence punctuation are retained with the last word of the sentence (for eg: moment.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ï»¿one', 'morning', 'when', 'gregor', 'samsa', 'woke', 'from', 'troubled', 'dreams', 'he', 'found', 'himself', 'transformed', 'in', 'his', 'bed', 'into', 'a', 'horrible', 'vermin', 'he', 'lay', 'on', 'his', 'armourlike', 'back', 'and', 'if', 'he', 'lifted', 'his', 'head', 'a', 'little', 'he', 'could', 'see', 'his', 'brown', 'belly', 'slightly', 'domed', 'and', 'divided', 'by', 'arches', 'into', 'stiff', 'sections', 'the', 'bedding', 'was', 'hardly', 'able', 'to', 'cover', 'it', 'and', 'seemed', 'ready', 'to', 'slide', 'off', 'any', 'moment', 'his', 'many', 'legs', 'pitifully', 'thin', 'compared', 'with', 'the', 'size', 'of', 'the', 'rest', 'of', 'him', 'waved', 'about', 'helplessly', 'as', 'he', 'looked', 'whats', 'happened', 'to', 'me', 'he', 'thought', 'it', 'wasnt', 'a', 'dream', 'his', 'room', 'a', 'proper', 'human']\n"
     ]
    }
   ],
   "source": [
    "#Remove punctuation from each word\n",
    "\n",
    "import string\n",
    "\n",
    "# create a mapping table\n",
    "table= str.maketrans('','',string.punctuation)\n",
    "strip = [ w.translate(table) for w in words]\n",
    "print(strip[:100])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Python provides a constant called string.punctuation that provides a great list of punctuation characters. \n",
    "- Python offers a function called translate() that will map one set of characters to another.\n",
    "- We can use the function maketrans() to create a mapping table. We can create an empty mapping table, but the third argument of this function allows us to list all of the characters to remove during the translation process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Tokenization and Cleaning with NLTK (Natural Language Toolkit)\n",
    "\n",
    "NLTK pipeline for text processing \n",
    "- Load the raw text.\n",
    "- Split into tokens.\n",
    "- Convert to lowercase.\n",
    "- Remove punctuation from each token.\n",
    "- Filter out remaining tokens that are not alphabetic.\n",
    "- Filter out tokens that are stop words.\n",
    "- Stem words (stemming refers to the process of reducing each word to its toot or base)\n",
    "    - For example “fishing,” “fished,” “fisher” all reduce to the stem “fish.”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLTK Downloader\n",
      "---------------------------------------------------------------------------\n",
      "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
      "---------------------------------------------------------------------------\n",
      "Downloader> q\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK provides the sent_tokenize() function to split text into sentences <br>\n",
    "NLTK provides a function called word_tokenize() for splitting strings into tokens (nominally words).\n",
    " - It splits tokens based on white space and punctuation. \n",
    " - For example, commas and periods are taken as separate tokens. \n",
    " - Contractions are split apart (e.g. “What’s” becomes “What” “‘s“). \n",
    " - Quotes are kept etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ï', 'morning', 'when', 'Gregor', 'Samsa', 'woke', 'from', 'troubled', 'dreams', 'he', 'found', 'himself', 'transformed', 'in', 'his', 'bed', 'into', 'a', 'horrible', 'vermin', 'He', 'lay', 'on', 'his', 'back', 'and', 'if', 'he', 'lifted', 'his', 'head', 'a', 'little', 'he', 'could', 'see', 'his', 'brown', 'belly', 'slightly', 'domed', 'and', 'divided', 'by', 'arches', 'into', 'stiff', 'sections', 'The', 'bedding', 'was', 'hardly', 'able', 'to', 'cover', 'it', 'and', 'seemed', 'ready', 'to', 'slide', 'off', 'any', 'moment', 'His', 'many', 'legs', 'pitifully', 'thin', 'compared', 'with', 'the', 'size', 'of', 'the', 'rest', 'of', 'him', 'waved', 'about', 'helplessly', 'as', 'he', 'looked', 'What', 'happened', 'to', 'me', 'he', 'thought', 'It', 'was', 'a', 'dream', 'His', 'room', 'a', 'proper', 'human', 'room']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "#split to words\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "#remove tokens that are not alphabetic\n",
    "words = [word for word in tokens if word.isalpha()]\n",
    "print(words[:100])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ï', '»', '¿one', 'morn', ',', 'when', 'gregor', 'samsa', 'woke', 'from', 'troubl', 'dream', ',', 'he', 'found', 'himself', 'transform', 'in', 'hi', 'bed', 'into', 'a', 'horribl', 'vermin', '.', 'he', 'lay', 'on', 'hi', 'armour-lik', 'back', ',', 'and', 'if', 'he', 'lift', 'hi', 'head', 'a', 'littl', 'he', 'could', 'see', 'hi', 'brown', 'belli', ',', 'slightli', 'dome', 'and', 'divid', 'by', 'arch', 'into', 'stiff', 'section', '.', 'the', 'bed', 'wa', 'hardli', 'abl', 'to', 'cover', 'it', 'and', 'seem', 'readi', 'to', 'slide', 'off', 'ani', 'moment', '.', 'hi', 'mani', 'leg', ',', 'piti', 'thin', 'compar', 'with', 'the', 'size', 'of', 'the', 'rest', 'of', 'him', ',', 'wave', 'about', 'helplessli', 'as', 'he', 'look', '.', '``', 'what', \"'s\"]\n"
     ]
    }
   ],
   "source": [
    "#Load data \n",
    "filename=r\"C:\\Users\\divyakamat\\data\\DataSets\\nlp\\metamorphosis_clean.txt\"\n",
    "with open(filename,'rt') as file:\n",
    "    text = file.read()\n",
    "    \n",
    "#split the words into tokens\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "#convert to lower case\n",
    "tokens = [word.lower() for word in tokens]\n",
    "\n",
    "#Remove punctuations within the tokens\n",
    "\n",
    "import string\n",
    "table = str.maketrans('','',string.punctuation)\n",
    "strip = [w.translate(table) for w in tokens]\n",
    "\n",
    "#Remove other tokens that are not alphabetic\n",
    "words = [word for word in strip if word.isalpha()]\n",
    "\n",
    "#filter out stop words like a , to , this etc\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "words = [w for w in words if not w in stop_words]\n",
    "\n",
    "#Stemming of words\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "porter = PorterStemmer()\n",
    "stemmed = [porter.stem(word) for word in tokens]\n",
    "\n",
    "print(stemmed[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the source text for this tutorial was reasonably clean to begin with, we skipped many concerns of text cleaning that you may need to deal with in your own project. <br>\n",
    "\n",
    "Here is a short list of additional considerations when cleaning text:<br>\n",
    "\n",
    "- Handling large documents and large collections of text documents that do not fit into memory.\n",
    "- Extracting text from markup like HTML, PDF, or other structured document formats.\n",
    "- Transliteration of characters from other languages into English.\n",
    "- Decoding Unicode characters into a normalized form, such as UTF8.\n",
    "- Handling of domain specific words, phrases, and acronyms.\n",
    "- Handling or removing numbers, such as dates and amounts.\n",
    "-  Locating and correcting common typos and misspellings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization\n",
    "Lemmatization is an alternative approach from stemming to removing inflection\n",
    "\n",
    "Lemmazation is a more intensive and therefore slower process, but more accurate. Stemming may be more useful in queries for databases whereas lemmazation may work much better when trying to determine text sentiment.\n",
    "\n",
    "from nltk.stem.wordnet import WordNetLemmatizer <br>\n",
    "lem = WordNetLemmatizer()\n",
    "\n",
    "from nltk.stem.porter import PorterStemmer <br>\n",
    "stem = PorterStemmer()\n",
    "\n",
    "word = \"multiplying\" <br>\n",
    "lem.lemmatize(word, \"v\") <br>\n",
    ">> \"multiply\" \n",
    "stem.stem(word) <br>\n",
    ">> \"multipli\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "https://machinelearningmastery.com/clean-text-machine-learning-python/\n",
    "\n",
    "https://towardsdatascience.com/pre-processing-in-natural-language-machine-learning-898a84b8bd47\n",
    "\n",
    "https://medium.com/@dobko_m/nlp-text-data-cleaning-and-preprocessing-ea3ffe0406c1\n",
    "\n",
    "https://www.theschool.ai/courses/data-lit/lessons/cleaning-text-data/\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [py36]",
   "language": "python",
   "name": "Python [py36]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
